Running:
src/main.py --output_dir experiments --data_dir data/SMPSKLearn --name SMPModelSKLearn --comment 2024-10-08 --load_model SMPModelSkLearn/checkpoints/model_last.pth --batch_size 128 --normalization standardization --test_pattern TEST --model transformer --d_model 128 --data_class csv --data_window_len 350 --num_heads 8 --num_layers 3 --dropout 0.1 --mask_mode separate --mask_distribution geometric --test_only testset --gpu -1 --epochs 300

Using device: cpu
Loading and preprocessing data ...
2879 nan values in data/SMPSKLearn\SMP_TEST.csv will be replaced by 0
1 nan values in data/SMPSKLearn\SMP_TRAIN.csv will be replaced by 0
2879 nan values in data/SMPSKLearn\SMP_TEST.csv will be replaced by 0
44 samples may be used for training
11 samples will be used for validation
9 samples will be used for testing
Creating model ...
Model:
TSTransformerEncoder(
  (project_inp): Linear(in_features=16, out_features=128, bias=True)
  (pos_enc): FixedPositionalEncoding(
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerBatchNormEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): TransformerBatchNormEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): TransformerBatchNormEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=256, out_features=128, bias=True)
        (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (output_layer): Linear(in_features=128, out_features=16, bias=True)
  (dropout1): Dropout(p=0.1, inplace=False)
)
Total number of parameters: 401680
Trainable parameters: 401680
